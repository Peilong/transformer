
\section{Introduction}


Chip Multi-Processors (CMPs) have become the mainstream processors for
mobile, desktop, and cloud computing platforms due to their power
budget as well as their limits in terms of clock frequency scaling. While
thread-level parallelism can be well leveraged with CMPs
for throughput oriented applications \cite{CMP05}, the single-thread performance of
a processor still remains important to time critical workloads. Therefore,
various ASIC-based accelerators are utilized to improve single-thread performance. 

However, ASIC based on-chip accelerators do not respond well to
emerging dynamic workloads where applications come and go either on
demand or in an unpredictable manner. For example, the user of a smartphone may
install or uninstall apps at any time, leading to a changing mixture
of applications. A cloud computing platform such as Amazon EC2
\cite{amazon-ec2} can execute a variety of workloads submitted at arbitrary points in time from a user's Virtual Machine (VM) 
 including the Web \cite{Chen:2012jo}, data mining \cite{ec2-datamining}, as well as DNA sequencing
\cite{ec2-dna}.  The built-in static accelerators may not be needed for extended periods of time
or for certain workload mixes.  As a result, the static accelerators remain dormant during these time periods, 
culminating in the waste of silicon resources.

A complementary approach consists of using off-chip programmable hardware
accelerators such as GPUs and/or FPGAs to speed up complex workloads
\cite{GPUFPGA, fpga-acc}. However, due to the nature of conventional system architectures, 
such hardware acceleration units do not meet the stringent time requirements 
of delay-sensitive workloads. The aforementioned units are typically
connected to the processor cores through off-chip interconnects
(e.g., PCIe, QPI \cite{intel-qpi} or by way of HyperTransport
\cite{amd-hypertransport}), each of which has serious latency limits and therefore results in
significant delays with regards to the transfer of data. Thus, even with their extensive
parallel processing capabilities, FPGAs and GPUs are challenged to produce the anticipated levels
of increased speed.  
%Their performance advantages
%and flexibility in reprogramming cannot be harnessed to the full
%extent until an architectural change occurs on their interconnects to
%cores.

Leveraging on-chip accelerators in existing as well as new applications is a
non-trivial undertaking given the prohibitive redevelopment costs.  
%Allowing an applications' execution flow to be transparently directed to accelerators
%without any user intervention remains an interesting but unsolved
%issue. 
As new instructions are typically added to SoC
accelerators, rewriting and recompiling the application's code is
a requirement. Fine-grained accelerations also require compiler support \cite{Govindaraju:2012fn}. As the IPs for
FPGAs and programs developed for GPUs continually increase, it becomes more plausible
to incorporate them through coarse-grained acceleration. For example, an entire function, such as 3DES encryption,
can be sped up as an element of an existing software library, such as libopenssl. A library based
approach facilitates acceleration without incurring development or redevelopment costs. 

The power efficiency \cite{hamada09,thomas09} and performance benefits
of recently developed programmable hardware has been driving the momentum of
heterogeneous computing platforms that combine general-purpose cores
and reconfigurable logic for high performance computing (HPC) applications, 
for example the Convey HC-1 \cite{brewer09} and the Cray XD1 \cite{Ulmer:2005vh}.
However, the workloads in many computing environments
are far more dynamic than those in the HPC domain. The
fusion of general-purpose cores with programmable logic for
power-efficient computing has been inadequately researched in terms of
the architecture trade-offs between performance and power, transparent
acceleration, and accelerator-aware scheduling, all of which are
critical to the practical deployment of programmable heterogeneous
architectures.

The {\em Transformer}, a heterogeneous architecture solution consisting of 
general purpose cores along with on-chip programmable accelerator logic, is a
viable means to address the challenges introduced by the dynamic workloads that exist within a number of
emerging scenarios. The aforementioned solution differs from existing architectures in several ways.
First, the architecture consists of on-chip programmable
accelerators with general purpose cores.  By sharing the memory hierarchy
with the cores, the accelerators are promoted to first-class citizens and therefore
reduce latency by means of efficient access to the data stored within each of the cores.  
Second, sharing an accelerator unit among multiple acceleration functions
improves the utilization of the chip resources. Optimal area resource
allocation to the cores and the accelerators leads to improved performance as well as improved
energy efficiency. Given a set of resource constraints, the heuristics proposed maximize the memory
bandwidth utilization and lead to speedup improvements with respect to unpredictable workload
mixtures. Third, the architecture
enables transparent acceleration with novel middleware support,
significantly reducing deployment costs.  It is worth noting that,
while the programmable accelerator logic in the proposed architecture
could potentially utilize GPU based coprocessors \cite{intel-gpu}, the focus of this paper
is upon FPGA-style reconfigurable logic due to the large set of existing IP
designs available for FPGAs and their improved power
efficiency. Nevertheless, the middleware and the scheduling algorithms
studied in this body of research are generic enough to benefit either
implementation.

We make the following contributions in this work:
\begin{enumerate}

\item We present a deployable heterogeneous architecture with run-time
  programmable on-chip accelerators. Our study evaluates the
  performance benefits of on-demand accelerators, and provides insight into how different parameters affect the performance and the power efficiency of the heterogeneous architecture.

\item We design a suite of middleware and scheduling algorithms for
  supporting transparent acceleration, which is an enabling factor for
  the wide deployment of accelerators, but not specifically addressed in
  prior research. To the best of our knowledge, our work is the first
  to address this issue and present a viable solution.

\item We characterize the power consumption of the proposed
  heterogeneous architecture with industrial level power modeling
  tools. This study gives important insights into the trade-offs between scalability and
  power-performance.

\end{enumerate}

The paper is organized as follows. Section \ref{sec_related} discusses
prior work related to our research. Section \ref{sec_arch} describes
the proposed programmable heterogeneous architecture, followed by the
transparent acceleration mechanisms detailed in Section
\ref{sec_transacc}. We present the accelerator-aware scheduling and
accelerator combination algorithms in Section
\ref{sec_runtime_reconfig}. The performance evaluation methodology along with the
experimental results are presented in Section \ref{sec_perf}. Finally, the paper is
concluded within Section \ref{sec_concl}.

 \if 0 FPGA as a
co-processing unit, has demonstrated the ability to speed up a variety
of applications, such as image processing \cite{imageacc}, data mining
\cite{data-mining-ref}, bioinformatics \cite{bioacc1} \cite{bioacc2},
navigation \cite{naviacc} and encryption/decryption
\cite{encryptionacc}. As another popular alternative acceleration
method, GPGPUs are less expensive and have higher memory bandwidth and
a larger number of programmable cores with thousands of hardware
threads than FPGA. So if the data to be processed can be simply
divided into many parallel trunks without any dependency or shared
data, it can be easily processed though the GPU
optimizations. However, there are several limitations of GPU that may
severely affect the computing performance on threaded GPU
platform. First, it is usually difficult to find how much data
parallelism lies in the application because no aspects of GPUs are
transparent to programmers \cite{microsoft06}. Second, scatter and
gather are two basic operations performed by GPU, which will also
introduce a gigantic amount of memory access latency and degradation
of memory bandwidth because of their access randomness
\cite{GPUlimit1}. Third, there are many domains of application with
large data dependency that are not optimized well on GPU
\cite{GPUlimit2}.  \fi
