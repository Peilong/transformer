\section{Related Work}
\label{sec_related}
Cong et al. in \cite{accrich,cong-islped12,cong-saw11} propose a
heterogeneous architecture called CHARM with loosely-coupled on-chip
accelerators. By composing the accelerator building blocks, the
architecture can dynamically speed up medical imaging benchmarks (up
to 3.7x) and show energy consumption reduction by up to 4.7x.  In
spite of its effectiveness in computation acceleration, this
accelerator-rich architecture introduces new accelerator instructions and
requires recompiling applications, lacking the run-time
reconfigurability.  Garcia et al. propose software based kernel sharing in a
multiprocessor system with reconfigurable hardware
	\cite{Garcia:2008iy}. This concept is similar with CHARM as both
of them emphasize the strategies on how to best utilize the existing
accelerator.

Our work differs from CHARM \cite{accrich,cong-islped12,cong-saw11} as
follows: (a) Transformer is targeted at cloud and mobile applications
where the execution environment has to handle dynamics of the
workloads as they arrive and depart at unpredictable time. So
Transformer focuses on run-time profiling workloads and reprogramming
acceleration functions accordingly. CHARM, on the other hand,
emphasizes on sharing the existing accelerators among software
threads, lacking the ability of coping with emerging workloads at
run-time; (b) Transformer incorporates a centralized reconfigurable
logic, instead of distributed fine-grained accelerator blocks in
\cite{accrich}, to improve the area utilization. The chip
resource dedicated to the acceleration logic can be re-programmed and
shared by multiple accelerator engines for different functions, thanks
to the partial reconfiguration technology.  We develop heuristics to
combining accelerators on-demand for maximizing memory bandwidth
utilization or speedup effect; and (c) Transformer has a set of
middleware innovations that avoid rewriting user applications with a
library-oriented coarse-grained approach. This is supported by the
growing set of available soft IP cores for reconfigurable logics
\cite{opencores,design-reuse,free-ip}. On the contrary, CHARM requires
rewriting user code.

Govindaraju et al. propose {\em DySER} with both functionality and
data parallelism specialization \cite{Govindaraju:2012fn,Govindaraju:HPCA11}. DySER
provides the feasibility of dynamically specializing execution
resources and creating various data path for parallelizable
``hot-spot'' in workload, and thus improve the performance and energy
efficiency. {\em Transformer}, in another research direction, aims at
coarse-grained acceleration at the level of a library function. {\em
  DySER} relies on a compiler to partition hot spots into compute and
data subregions and offload to dynamically formed functional
units. Without compiler assistance, {\em Transformer} contains
hardware and middleware to perform run-time reconfiguration to avoid
recompilation, supporting the acceleration of closed-source
executables.

{\em Garp} \cite{Garp:1997,Garp:2000} and {\em Transformer} target on
the same direction: that is, improving computation performance and power
efficiency with FPGA-based parallelization. However, the two
architectures focus on different granularities of parallelism. Similar
to {\em DySER}, {\em Garp} targets on a finer granularity with the
optimization of basic operations to achieve instruction level
parallelism. Such parallelization heavily relies on a compiler because the compiler needs to
precisely identify the potential parallelizable code and map the code
to an acceleration block in a way that its performance could be
improved. The penalty of imprecise identification is significant at such instruction level granularity.
%Otherwise, the compiler optimized code might compromise the
%performance. 
Our approach focuses on the optimization at library (function) level,
considering the data sharing and computation in a global
view, so it usually guarantees performance improvement. We always know
an optimized version on FPGA outperforms the CPU version of the same
function with tens or even hundreds of speedup. We just need to make
sure to plug in the right accelerator at the right time to tackle with
the dynamics of the workload. The profiling and scheduling methods are applicable to fine-granularity acceleration too.

Other works such as HiPPAI \cite{Stillwell:2009if} and EXOCHI
\cite{Wang:2007bc} propose new programming environment or interface
for hardware accelerated SoC platform. HiPPAI abstracts a layer of
accelerator interface in OS to schedule task either onto the
accelerator or to the general purpose core. However, this layer of
abstraction lacks the awareness of the run-time dynamics on the
accelerator. In contrast, we propose a reconfiguration controller to
keep track of the usage information on logic and make 
efficient use of the logic resource, without requiring reprogramming. 

Towards high performance big data applications, LINQits is proposed as
a flexible hardware template that can be mapped onto programmable
logic or ASICs in a heterogeneous system-on-chip for a mobile device
or a server \cite{Chung:2013:LBD:2485922.2485945}. LINQits accelerates a domain-specific query
language called LINQ, and LINQits requires applications (re-)written with
LINQ for taking advantage of hardware acceleration. 

While the LINQits architecture has similarities with our proposed {\em
  Transformer} architecture in a ``core + FPGA'' form, there exist
notable differences between them: (a) {\em Transformer} is a
generic architecture not tied to a particular application domain
whereas LINQits is designed and optimized for big data applications; 
and (b) no rewriting or recompiling application is needed in {\em Transformer} while
LINQits relies on rewriting applications with LINQ language
constructs, which are not applicable for non-database applications.

Huang et al. propose an aggregate gain algorithm (AG)
\cite{Huang:2009hs} to predict future acceleration requirement of the
workload. This prediction mechanism may work properly on some specific
domain of computing, however, cloud workloads are too dynamic to be
accurately predicted. Also in \cite{Huang:2009hs}, cache coherency is not considered and the
number of LUTs is the only resource constraint. {\em Transformer}
maintains coherency on both cores' caches and accelerators' local
memory and considers constraints of all logic resources (LUT, BRAM,
SLICE, etc.).


In \cite{Mignolet:2003gr}, Mignolet et al. introduce a feasible way of
relocating tasks between software path and accelerated hardware
path. They implement a new OS (OS4RS) which contains a hardware
abstraction layer and a communication interface to enable OS tasking
scheduling between software and hardware. 
However, {\em Transformer} does not require changes to OS
layer. Instead, transparent acceleration in {\em Transformer} is
achieved with a wrapper library which resides at the middleware layer,
improving the portability of the design.


Supercomputers pursue the highest computation performance
\cite{Ulmer:2005vh} with supports of FPGAs. For instance, each node
(six nodes in a chassis) in Cray XD1 integrates one Xilinx Virtex-4
connected via a RadipArray Interconnect to four memory banks shared
with two AMD Opteron processors. Though both providing run-time
reprogrammability, Cray XD1 differs from {\em Transformer} in two
aspects: (a) reconfigurable logic on Cray XD1 is an off-chip
coprocessor which communicates with CPUs under the control of Rapid
Array Processors whereas {\em Transformer} integrates reconfigurable
logics onto the SoC, permitting on-chip data sharing with cores and (b)
to utilize the coprocessors on Cray XD1, users need to apply the
vendor-provided FPGA Linux API into their software design. In
contrast, {\em Transformer} provides a transparent scheme of invoking
accelerators.

As an ``extreme" form on fixed specialization, conservation cores
\cite{Venkatesh:2010:CCR:1735970.1736044,Venkatesh:2010:CCR:1736020.1736044,Venkatesh:2010:CCR:1735971.1736044},
or c-cores, are specialized processors that focus only on energy and
energy-delay efficiency rather than improving performance. C-cores
provides a promising reduction of energy consumption (up to
16.0$\times$ for specific functions and up to 2.1$\times$ for whole
application), which is comparable with {\em Transformer}'s energy
efficiency (up to 6.9$\times$), at the risk of compromising the
performance. However, we argue that QoS is one of the most important
concerns for most of the cloud service provider, that is, providing
the best user experience. Thus, the balance between performance and
power consumption should be carefully considered. {\em Transformer}
provides a solution that considers both performance and power aspects
in cloud services. In addition, the service provider typically does
not have access to the source code used or owned by the customers in a
cloud computing environment. This makes c-core ineffective in such scenarios. {\em Transformer}'s middleware approach based on wrapper library imposes no requirements on source code access, therefore is feasible for a wide range of applications. 

Given the reprogramming/partial-reconfiguration costs of FPGA-type of
logics (in the order of tens or hundreds of milliseconds depending on
the size of bit streams \cite{Liu:2009ie}), the benefits of adapting
the accelerator functions at run-time must outweigh such
costs. Characterization on cloud computing workloads in
\cite{CloudWorkload} has demonstrated that the lifetime of various
applications ranges from 300 to 86400 seconds, implying that an
accelerated function would be used for a considerable duration.  This
validates our rationale of reprogramming accelerators with different
functions on-demand.

Our proposed architecture is built atop of partial reconfiguration
technology such as XILINX Dynamic Partial Reconfiguration (DPR)
\cite{PRUserGuide} to reduce reconfiguration time while not
interrupting the operation of other logics. For example, in Xilinx DPR
design process, a library of hardware modules called Partially
Reconfigurable Modules (PRMs) and their corresponding Partial Bit
Streams (PBS) are created in advance. 
%The circuits that are not configurable during runtime are called Static Region. 
During runtime, PBSs are configured to a region on device called
Partial Reconfigurable Region (PRR) through a primitive configuration
module known as Internal Configuration Access Port (ICAP)
\cite{Hansen:2011dt,Liu:2009ie,McDonald:2008ec}. We leverage ICAP-like
structure to reprogram the configurable logic on demand.



